# -*- coding: utf-8 -*-
"""Brest Cancer ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V9BAfcN6rIfXBRpQFwkiv5tQwn5f2nKR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer
cancer_dataset=load_breast_cancer()

cancer_dataset

type(cancer_dataset)

cancer_dataset.keys()

cancer_dataset['data']

type(cancer_dataset['data'])

cancer_dataset['target']    #malignant --positive 0
                              #benign -- negative 1

print(cancer_dataset['DESCR'])

print(cancer_dataset['filename'])

cancer_df=pd.DataFrame(np.c_[cancer_dataset['data'],cancer_dataset['target']],
                       columns=np.append(cancer_dataset['feature_names'],['target']))

cancer_df.to_csv('breast_cancer_dataframe.csv')

cancer_df.head(6)

cancer_df.tail(10)

cancer_df.info()

cancer_df.describe()

cancer_df.isnull().sum()

sns.pairplot(cancer_df,hue='target',vars=['mean radius','mean texture','mean perimeter','mean area','mean smoothness'])

sns.countplot(cancer_df['target'])

plt.figure(figsize=(20,8))
sns.countplot(cancer_df['mean radius'])

plt.figure(figsize=(16,9))
sns.heatmap(cancer_df)

plt.figure(figsize=(20,20))
sns.heatmap(cancer_df.corr(),annot=True,cmap='coolwarm',linewidths=5)

cancer_df2 = cancer_df.drop(['target'], axis=1)
print("The shape of 'cancer_df2' is : ", cancer_df2.shape)

cancer_df.corr()

# visualize correlation barplot
plt.figure(figsize = (16,5))
ax = sns.barplot(cancer_df2.corrwith(cancer_df.target).index, cancer_df2.corrwith(cancer_df.target))
ax.tick_params(labelrotation = 90)

cancer_df.corrwith(cancer_df.target).index

X=cancer_df.drop(['target'],axis=1)
X.head(5)

Y=cancer_df['target']
Y.head(5)

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=5)

X_train

X_test

Y_train

Y_test

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train_sc=sc.fit_transform(X_train)
X_test_sc=sc.transform(X_test)

"""fit computes the mean and std to be used for later scaling. (jsut a computation), nothing is given to you.

"transform" uses a previously computed mean and std to autoscale the data (subtract mean from all values and then divide it by std).

"fit_transform" does both at the same time
"""

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score

"""## **Support Vector Machine**"""

from sklearn.svm import SVC
svc_classifier= SVC()
svc_classifier.fit(X_train,Y_train)
Y_pred_svc= svc_classifier.predict(X_test)
accuracy_score(Y_test,Y_pred_svc)

svc_classifier2=SVC()
svc_classifier2.fit(X_train_sc,Y_train)
Y_pred_svc_sc=svc_classifier2.predict(X_test_sc)
accuracy_score(Y_test,Y_pred_svc_sc)

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr_classifier=LogisticRegression(random_state=51, penalty='l1')
lr_classifier.fit(X_train,Y_train)
Y_pred_lr=lr_classifier.predict(X_test)
accuracy_score(Y_test,Y_pred_lr)

from sklearn.linear_model import LogisticRegression
lr_classifier = LogisticRegression(random_state = 51, penalty = 'l2')
lr_classifier.fit(X_train, Y_train)
Y_pred_lr = lr_classifier.predict(X_test)
accuracy_score(Y_test, Y_pred_lr)

# Train with Standard scaled Data
lr_classifier2 = LogisticRegression(random_state = 51, penalty = 'l1')
lr_classifier2.fit(X_train_sc, Y_train)
Y_pred_lr_sc = lr_classifier.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_lr_sc)

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn_classifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
knn_classifier.fit(X_train,Y_train)
Y_pred_knn=knn_classifier.predict(X_test)
accuracy_score(Y_test,Y_pred_knn)

# Train with Standard scaled Data
knn_classifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knn_classifier2.fit(X_train_sc, Y_train)
Y_pred_knn_sc = knn_classifier2.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_knn_sc)

"""## Naive Bayes classifier"""

from sklearn.naive_bayes import GaussianNB
nb_classifier=GaussianNB()
nb_classifier.fit(X_train,Y_train)
Y_pred_nb=nb_classifier.predict(X_test)
accuracy_score(Y_test,Y_pred_nb)

from sklearn.naive_bayes import GaussianNB
nb_classifier2=GaussianNB()
nb_classifier2.fit(X_train_sc,Y_train)
Y_pred_nb_sc=nb_classifier2.predict(X_test_sc)
accuracy_score(Y_test,Y_pred_nb_sc)

"""## Decision Tree classifier"""

# Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)
dt_classifier.fit(X_train, Y_train)
Y_pred_dt = dt_classifier.predict(X_test)
accuracy_score(Y_test, Y_pred_dt)

# Train with Standard scaled Data
dt_classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)
dt_classifier2.fit(X_train_sc, Y_train)
Y_pred_dt_sc = dt_classifier2.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_dt_sc)

"""## Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 51)
rf_classifier.fit(X_train, Y_train)
Y_pred_rf = rf_classifier.predict(X_test)
accuracy_score(Y_test, Y_pred_rf)

# Train with Standard scaled Data
rf_classifier2 = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 51)
rf_classifier2.fit(X_train_sc, Y_train)
Y_pred_rf_sc = rf_classifier2.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_rf_sc)

"""## Adaboost Classifier"""

from sklearn.ensemble import AdaBoostClassifier
adb_classifier = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 200),
                                    n_estimators=2000,
                                    learning_rate=0.1,
                                    algorithm='SAMME.R',
                                    random_state=1,)
adb_classifier.fit(X_train, Y_train)
Y_pred_adb = adb_classifier.predict(X_test)
accuracy_score(Y_test, Y_pred_adb)

adb_classifier2 = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 200),
                                    n_estimators=2000,
                                    learning_rate=0.1,
                                    algorithm='SAMME.R',
                                    random_state=1,)
adb_classifier2.fit(X_train_sc, Y_train)
Y_pred_adb_sc = adb_classifier2.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_adb_sc)

"""## XGBoost Classifier"""

from xgboost import XGBClassifier
xgb_classifier=XGBClassifier()
xgb_classifier.fit(X_train, Y_train)
Y_pred_xgb = xgb_classifier.predict(X_test)
accuracy_score(Y_test, Y_pred_xgb)

xgb_classifier2=XGBClassifier()
xgb_classifier2.fit(X_train_sc, Y_train)
Y_pred_xgb_sc = xgb_classifier2.predict(X_test_sc)
accuracy_score(Y_test, Y_pred_xgb_sc)

"""## XGBoost Parameter Tuning Randomized Search"""

# XGBoost classifier most required parameters
params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ] 
}

# Randomized Search
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(xgb_classifier, param_distributions=params, scoring= 'roc_auc', n_jobs= -1, verbose= 3)
random_search.fit(X_train, Y_train)

random_search.best_params_

random_search.best_estimator_

xgb_classifier_pt=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0.1,
              learning_rate=0.15, max_delta_step=0, max_depth=5,
              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)

xgb_classifier_pt.fit(X_train,Y_train)
Y_pred_xgb_pt = xgb_classifier_pt.predict(X_test)
accuracy_score(Y_test,Y_pred_xgb_pt)

# from sklearn.model_selection import GridSearchCV
# grid_search = GridSearchCV(xgb_classifier, param_grid=params, scoring= 'roc_auc', n_jobs= -1, verbose= 3)
# grid_search.fit(X_train, Y_train)

"""## Confusion Matrix"""

cm = confusion_matrix(Y_test,Y_pred_xgb_pt)
plt.title('Heat map of confusion matrix',fontsize=20)
sns.heatmap(cm,annot=True,linecolor='white')
plt.show()

###### New Section
# there is no type 2 error  
# 2 person aise hai jinko bataya jaa rha hai ki cancer hai but unko nhi hai
# so it is best

print(classification_report(Y_test,Y_pred_xgb_pt))

# Cross validation for checking overfitting or underfitting
# if(accuracy of model> accuracy of cross validation) --> overfitting otherwise underfitting
# from sklearn.model_selection import cross_val_score
# cross_validation = cross_val_score(estimator = xgb_model_pt2, X = X_train_sc, y = y_train, cv = 10)
# print("Cross validation of XGBoost model = ",cross_validation)
# print("Cross validation of XGBoost model (in mean) = ",cross_validation.mean())

from sklearn.model_selection import cross_val_score
cross_validation = cross_val_score(estimator = xgb_classifier_pt, X = X_train_sc,y = Y_train, cv = 10)
print("Cross validation accuracy of XGBoost model = ", cross_validation)
print("\nCross validation mean accuracy of XGBoost model = ", cross_validation.mean())

